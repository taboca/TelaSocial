http://github.com/taboca/TelaSocial/issues#issue/3

 In order to implement this we need to implement a data connection and feed proxy module and make this available to the widgets codes as part of the choreographer API. Today widget modules are responsible for loading and managing the information - they directly interact with Web content. This can be done today via using Google AJAX RSS feed API or the widget component can make a call using XHR. The new proposal is to have a proxy component in the middle which is responsible for loading RSS information, maintaining a data store and events coordination system so that widgets can receive nodes of information. With this approach a widget should be able to collect feed items from various pre-registered feed channels thus the effect of a rotating RSS ticker for various RSS data-sources will be possible.

jQuery jFeed plugin 

http://www.hovinne.com/blog/index.php/2007/07/15/132-jfeed-jquery-rss-atom-feed-parser-plugin

Architecture 
===

Register Feed 
Networking Cycles 
Context Store
Observers Registration
Events Dispatch 

mySQL store for items chunks and reference to the channel 
===
var file = Components.classes["@mozilla.org/file/directory_service;1"]
                     .getService(Components.interfaces.nsIProperties)
                     .get("ProfD", Components.interfaces.nsIFile);
file.append("my_db_file_name.sqlite");

var storageService = Components.classes["@mozilla.org/storage/service;1"]
                        .getService(Components.interfaces.mozIStorageService);
var mDBConn = storageService.openDatabase(file); // Will also create the file if it does not exist

Profile in XULRUnner is /root/.taboca/telasocial 

Store Pool 
===
* We want to Load the items and break them in flat nodes; Whicih is to separate the items from the channel other info and refer back to the channel from the items. Timestamp the time snapshot moment capture to each item

 FeedItem channelnsinfo, timecaptured, checkstate, pubDate and other keys exported to this upper level 
 
   Item Data 
   * item.title
   * item.link
   * item.description
   * item.updated
   * item.id

When RSS is loaded again, we check format all the items in prep work to put in the store and we try to put in the store if it is not there. So there it's a put insert if it is a diff = new elements only. 

Once we have the elements in the store we need to create documents that are aggregation points to contextualization. These docuemnts are representations in DOM so that we will be able to use selctors to get some collections and generate contextual events. A contextual event is a key to the selection for a given document. 

feeditem | event [domain="www.slashdot.com"] { 
-context-event: Artigo; 
} 

We also need this system to work as a queue and allow processing states over the nodes. We do not want to have a document that is a huge mix of the many items from the many feeds to live forever growing. While the data-base information may grow for a longer time, the document space is a temporary representation that can be result of certin type queries. One possible example is a query that gives you all the elements - probably whay we do not want. But a case we are intested can be feed items that are not yet associated with any stored/processed state/flag/context. For example picture that the data stored received 10 items from slashdot and also 10 items from cnn.com. Now we have a collection of 20 items in the store. We can then do a query looking for all the items that have a context = fresh. This context = fresh can be a key variable that is stored in the item at the time it first comes from the network and was inserted in the DB. So if all items in the first place get a property = context = fresh this means they are all fresh so the app will end up making a contextual yet temporary document results of a query context = fresh = 20 items. 

a possible rule for this can be 

feeditem | event [domain="www.slashdot.org"] [space.context="*.fresh"]
  -context-event: New_Article; 
} 

feeditem | event [domain="www.othermysite.org"] [space.context="*.fresh"]
  -context-event: New_Article; 
} 

An initial processing queue executes the checks and creates contextual events. We need to keep both items in the store and also contextual events - the indexes to the queries. This will allow us to quickly search for New_Article again if we want. But a question here is why we would want to access the New_Articles again if we have the event that is created as the data comes in. 

One importat need to keep a contextual event in the store is the fact that we want to annotate the ones we already have processed. So for an incoming list A,B,C that happens in t=0 we may end up with D,E,F in a t=1 state. So for the A,B,C a queue was created, the rule is true to all these let's say, so a contextual key event is created let's say: 

K0 = A,B,C
and also total store = A,B,C

When in t=1 the RSS feed brought B,C,D,E,F and then theoreticaly all these would end up generating a contextual event in the chain. So we need to avoid B,C,D to be in this match. For this reason we may want to check the if the items area already placed in the store or if they are placed in some recent contextual event. Checking a new item against a huge store is an option. 


So new_article should be dispatched against watchers of that. A watcher is an observer that will receive a document with the elements in it. So the query was already performed and the watcher ( possible a widget module ) will simply get a hook to the document that is the collection of ann new fresh items. Now a question is when to mark the items as non-fresh. One case to exercise is the case where a given visual module wants to display news from the both ( mashed ) feeds. So imagine a layout divided in two areas like this 

-----------------
|       |       |
|   a   |   b   |
|       |       |
-----------------

Both a and b are widgets that can display news. They both want to display the news. While widget a wants to show from top to the bottom, widget b wants to display from bottom to top. So if both are looking gor New_Article they will from time to time ( everytime a new item enters in the store / system ) get a call and a document. The call will be somrthing like A.doSomething(Document) where Document has a number of items. If we had an internal function marking the items as non fresh a possible problem would be b never receiving the elements. So one option here is to have the contextual document to create a snapshot of the elements in the store and keep that reference at the moment of the contextualization intact for the lifetime of operations performed by all possible watchers/consumers. So then then observer A is called and if A marks the elements as non fresh, the actuall store elements are marked as non fresh only for A, as if all the elements needed to receive an attribute that it is A.fresh=false. 

New_Article: * { 
 
   -event-observer: WidgetA;
   -action-rule: this.fresh=false; 

} 


So while A receiveds the element the action rule is fresh false for A-scope fresh variable. So this New_Article will never happen next time for A but it may happen for B. Eventually some further advanced mixing can occur if you let A see B scope etc. 

One thing is to consider is the storage of the events and the document states for each - the queries and the operations performed. So let's say that when a rule query is done, a document is created which is a link to each node. 






References
===

* https://developer.mozilla.org/en/DOM/Storage
* XULRunner profile - https://wiki.mozilla.org/XULRunner
